---
title: "p8105_hw5_wl2926"
writer: "Wenwen Li"
output: github_document
date: "2023-10-18"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidyverse)
library(ggpubr)
library(broom)
theme_set(theme_pubr() + theme(
  legend.position = "right"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis",
  scipen = 999)
```

## Problem 1

For this problem, we are interested in data gathered and made public by _The Washington Post_ on homicides in 50 large U.S. cities. The code chunk below imports and cleans the data.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ "unsolved",
      disposition == "Open/No arrest"        ~ "unsolved",
      disposition == "Closed by arrest"      ~ "solved"
    )
  ) %>% 
  filter(city_state != "Tulsa, AL") 
```

The resulting dataframe has `r nrow(homicide_df)` entries, on variables that include the victim name, race, age, and sex; the date the homicide was reported; and the location of the homicide. In cleaning, I created a `city_state` variable that includes both city and state, and a `resolution` variable to indicate whether the case was closed by arrest. I also excluded one entry in Tulsa, AL, which is not a major US city and is most likely a data entry error. 

In the next code chunk, I group within cities and summarize to produce the total number of homicides and the number that are solved. 

```{r}
city_homicide_df = 
  homicide_df %>% 
  select(city_state, disposition, resolution) %>% 
  group_by(city_state) %>% 
  summarize(
    hom_total = n(),
    hom_unsolved = sum(resolution == "unsolved"))
```

Focusing only on Baltimore, MD, I can use the `prop.test` and `broom::tidy` functions to obtain an estimate and CI of the proportion of unsolved homicides in that city. The table below shows those values.

```{r}
bmore_test = 
  prop.test(
    x = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_unsolved),
    n = filter(city_homicide_df, city_state == "Baltimore, MD") %>% pull(hom_total)) 

broom::tidy(bmore_test) %>% 
  knitr::kable(digits = 3)
```

Building on this code, I can use functions in the `purrr` package to obtain estimates and CIs for the proportion of unsolved homicides in each city in my dataset. The code below implements this analysis. 

```{r}
test_results = 
  city_homicide_df %>% 
  mutate(
    prop_tests = map2(hom_unsolved, hom_total, \(x, y) prop.test(x = x, n = y)),
    tidy_tests = map(prop_tests, broom::tidy)) %>% 
  select(-prop_tests) %>% 
  unnest(tidy_tests) %>% 
  select(city_state, estimate, conf.low, conf.high) %>% 
  mutate(city_state = fct_reorder(city_state, estimate))
```

Finally, I make a plot showing the estimate (and CI) of the proportion of unsolved homicides in each city.

```{r}
test_results %>% 
  mutate(city_state = fct_reorder(city_state, estimate)) %>% 
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

This figure suggests a very wide range in the rate at which homicides are solved -- Chicago is noticeably high and, given the narrowness of the CI, likely is the location of many homicides.

# Problem 2: Data from a longitudinal study.    

```{r}
#get the list of file names
file_names <- list.files(path = "data/",
                         pattern = ".csv",
                         full.names = TRUE)
file_names

```  

```{r}
#data importation and preparation
longitudinal_data <- tibble(file = file_names) %>%
  mutate(
    arm = str_extract(file, "(con|exp)"), #arm group names from file names
    subject_id = str_extract(file, "\\d+"),#subjectID as numeric part,file name
    data = purrr::map(file, read_csv)) %>% #import data
  unnest(data) %>%#expand data list into rows and columns
  select(subject_id, arm, everything()) %>%
  select(-file) #remove file names column
#view first few rows of resulting tibble
longitudinal_data[c(1:4, 16:20),]
```

```{r}
#convert data to long format for ease of plotting
spaghetti_plot_data <- longitudinal_data %>%
  pivot_longer(cols = starts_with("week_"), names_to = "week",
               values_to = "value") %>%
  mutate(week_numeric = as.numeric(sub("week_", "", week))) 

#spaghetti plot of subjects grouped by arm over time.
ggplot(spaghetti_plot_data, aes(x = week_numeric, y = value,
                              group = subject_id, color=subject_id)) +
  geom_line(linewidth=1.5) +
  facet_grid(~arm) +
  labs(title = "Spaghetti plot of observations over time",
       x = "Week",
       y = "Observation",
       color="Subject ID")

```
The experimental group's observations are increasing with time, while the observations for the control group appear to oscillate around the same values. 

# Problem 3: Effect size and power in study design. 
```{r}
#set random number generator seed
set.seed(1111)
#set design elements
n <- 30
sigma <- 5
alpha <- 0.05
num_datasets <- 5000
true_mu <- 0  #specify mu value
simulations_mu_0 <- tibble(
  true_mu = rep(true_mu, num_datasets),
  data = map(1:num_datasets, ~rnorm(n, mean = true_mu, sd = sigma)),
  test_result = map(data, ~broom::tidy(t.test(.x, mu = 0)))) %>%
  unnest(test_result) %>%
  mutate(reject_null = p.value < alpha,
         estimate_rejected = if_else(reject_null, estimate, NA_real_))
#calculate power for mu=0
power_mu_0 <- mean(simulations_mu_0$reject_null)
power_mu_0
#compute average estimate for mu
avg_estimate_mu_0 <- mean(simulations_mu_0$estimate)
avg_estimate_mu_0
#compute average estimate for rejected samples only
avg_estimate_rejected_mu_0 <- mean(simulations_mu_0$estimate_rejected, 
                                   na.rm = TRUE)
avg_estimate_rejected_mu_0
```